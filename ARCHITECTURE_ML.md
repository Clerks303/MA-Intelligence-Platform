# Architecture Modulaire ML - M&A Intelligence Platform

## ğŸ—ï¸ Vue d'ensemble

Cette architecture sÃ©pare clairement les responsabilitÃ©s entre l'API backend et le calcul des scores ML, permettant une scalabilitÃ© et une maintenabilitÃ© optimales.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    M&A Intelligence Platform                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   ğŸŒ Frontend    â”‚â—„â”€â”€â”€â”¤   ğŸš€ API Backend â”‚â—„â”€â”€â”€â”¤ğŸ’¾ Supabaseâ”‚   â”‚
â”‚  â”‚   (React)        â”‚    â”‚   (FastAPI)      â”‚    â”‚           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                   â”‚                   â–²         â”‚
â”‚                                   â”‚                   â”‚         â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚         â”‚
â”‚                          â”‚  â° Scheduler   â”‚          â”‚         â”‚
â”‚                          â”‚  (Celery/Cron)  â”‚          â”‚         â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚         â”‚
â”‚                                   â”‚                   â”‚         â”‚
â”‚                                   â–¼                   â”‚         â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚         â”‚
â”‚                          â”‚  ğŸ¤– ML Service     â”‚â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                          â”‚  (LightGBM/XGBoost)â”‚                 â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Composants Principaux

### 1. ğŸŒ API Backend (FastAPI)
**ResponsabilitÃ©s :**
- Authentification et autorisation
- CRUD des entreprises
- Scraping de donnÃ©es
- **Lecture des scores prÃ©-calculÃ©s**
- Interface REST/GraphQL

**Technologies :**
- FastAPI + Uvicorn
- SQLAlchemy + Alembic
- Redis (cache)
- Supabase client

### 2. ğŸ¤– ML Scoring Service
**ResponsabilitÃ©s :**
- EntraÃ®nement des modÃ¨les ML
- Calcul des scores M&A
- Feature engineering
- **Ã‰criture des scores dans la base**

**Technologies :**
- LightGBM, XGBoost, CatBoost
- scikit-learn, pandas
- MLflow (model registry)
- Supabase client (Ã©criture)

### 3. â° Scheduler Service
**ResponsabilitÃ©s :**
- DÃ©clenchement manuel des calculs
- Planification nocturne automatique
- Monitoring des tÃ¢ches ML
- Gestion des queues

**Technologies :**
- Celery + Redis
- Cron jobs
- APScheduler

### 4. ğŸ’¾ Base de DonnÃ©es (Supabase)
**Tables principales :**
```sql
-- Table principale des entreprises
companies (
  id, siren, nom_entreprise, 
  chiffre_affaires, secteur, ...
)

-- Table des scores ML (nouvelle)
ml_scores (
  id, company_id, 
  score_ma, score_croissance, score_stabilite,
  model_version, calculated_at, confidence
)

-- Table des modÃ¨les ML (nouvelle)
ml_models (
  id, name, version, 
  model_file_path, metrics, created_at
)
```

## ğŸ“ Structure de Fichiers ProposÃ©e

```
backend/
â”œâ”€â”€ api/                          # ğŸŒ API Backend (actuel)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/routes/
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”œâ”€â”€ requirements-api.txt      # DÃ©pendances API uniquement
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ ml-service/                   # ğŸ¤– ML Service (nouveau)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ models/              # ModÃ¨les ML
â”‚   â”‚   â”œâ”€â”€ features/            # Feature engineering
â”‚   â”‚   â”œâ”€â”€ training/            # Scripts d'entraÃ®nement
â”‚   â”‚   â”œâ”€â”€ scoring/             # Calcul des scores
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”œâ”€â”€ requirements-ml.txt      # DÃ©pendances ML uniquement
â”‚   â”œâ”€â”€ batch_scoring.py         # Script principal
â”‚   â””â”€â”€ train_models.py          # EntraÃ®nement
â”‚
â”œâ”€â”€ scheduler/                   # â° Scheduler (nouveau)
â”‚   â”œâ”€â”€ celery_app.py
â”‚   â”œâ”€â”€ tasks.py
â”‚   â”œâ”€â”€ cron_jobs.sh
â”‚   â””â”€â”€ requirements-scheduler.txt
â”‚
â”œâ”€â”€ shared/                      # ğŸ“¦ Code partagÃ©
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ models.py           # ModÃ¨les SQLAlchemy partagÃ©s
â”‚   â”‚   â””â”€â”€ supabase_client.py  # Client Supabase
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py         # Configuration partagÃ©e
â”‚   â””â”€â”€ utils/
â”‚
â””â”€â”€ docker-compose.yml           # Orchestration complÃ¨te
```

## ğŸ”„ Flux de DonnÃ©es

### Flux Principal (Lecture des Scores)
```
1. ğŸ‘¤ Utilisateur â†’ API Backend
2. ğŸš€ API Backend â†’ Supabase (SELECT ml_scores)
3. ğŸ’¾ Supabase â†’ API Backend (scores + metadata)
4. ğŸš€ API Backend â†’ Frontend (JSON avec scores)
```

### Flux ML (Calcul des Scores)
```
1. â° Scheduler â†’ ML Service (trigger)
2. ğŸ¤– ML Service â†’ Supabase (SELECT companies)
3. ğŸ¤– ML Service â†’ Feature Engineering
4. ğŸ¤– ML Service â†’ ModÃ¨les ML (LightGBM/XGBoost)
5. ğŸ¤– ML Service â†’ Supabase (INSERT/UPDATE ml_scores)
```

## ğŸ› ï¸ ImplÃ©mentation Technique

### 1. Modification de l'API Backend

**Nouveau service de scores :**
```python
# app/services/scoring_service.py
class ScoringService:
    async def get_company_scores(self, company_id: int):
        """RÃ©cupÃ¨re les scores prÃ©-calculÃ©s"""
        return await self.db.fetch_one(
            "SELECT * FROM ml_scores WHERE company_id = ?", 
            company_id
        )
    
    async def get_scores_batch(self, company_ids: List[int]):
        """RÃ©cupÃ¨re les scores pour plusieurs entreprises"""
        pass
```

**Route API modifiÃ©e :**
```python
# app/api/routes/companies.py
@router.get("/{company_id}/scores")
async def get_company_scores(company_id: int):
    scores = await scoring_service.get_company_scores(company_id)
    return {
        "company_id": company_id,
        "scores": scores,
        "last_calculated": scores.calculated_at,
        "model_version": scores.model_version
    }
```

### 2. ML Scoring Service

**Script principal :**
```python
# ml-service/batch_scoring.py
class MLScoringService:
    def __init__(self):
        self.models = self.load_models()
        self.supabase = create_supabase_client()
    
    async def calculate_scores_batch(self, batch_size=1000):
        """Calcule les scores pour toutes les entreprises"""
        companies = await self.get_companies_to_score()
        
        for batch in self.chunk(companies, batch_size):
            features = self.engineer_features(batch)
            scores = self.predict_scores(features)
            await self.save_scores(scores)
    
    def predict_scores(self, features):
        """Applique les modÃ¨les ML"""
        return {
            'score_ma': self.models['lightgbm'].predict(features),
            'score_croissance': self.models['xgboost'].predict(features),
            'score_stabilite': self.models['catboost'].predict(features)
        }
```

### 3. Scheduler avec Celery

**Configuration Celery :**
```python
# scheduler/celery_app.py
from celery import Celery
from celery.schedules import crontab

app = Celery('ml_scheduler')

app.conf.beat_schedule = {
    'nightly-ml-scoring': {
        'task': 'tasks.run_ml_scoring',
        'schedule': crontab(hour=2, minute=0),  # 2h du matin
    },
}

# scheduler/tasks.py
@app.task
def run_ml_scoring():
    """TÃ¢che Celery pour lancer le scoring ML"""
    import subprocess
    result = subprocess.run([
        'python', '/app/ml-service/batch_scoring.py'
    ])
    return result.returncode
```

## ğŸ³ DÃ©ploiement Docker

```yaml
# docker-compose.yml
version: '3.8'
services:
  api-backend:
    build: ./api
    ports: ["8000:8000"]
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
    depends_on: [redis]
  
  ml-service:
    build: ./ml-service
    environment:
      - SUPABASE_URL=${SUPABASE_URL}
    volumes:
      - ./models:/app/models
  
  scheduler:
    build: ./scheduler
    depends_on: [redis, ml-service]
  
  redis:
    image: redis:7-alpine
```

## ğŸ“Š Gestion des ModÃ¨les ML

### MLflow Integration
```python
# ml-service/app/models/model_manager.py
import mlflow

class ModelManager:
    def load_production_models(self):
        """Charge les modÃ¨les en production depuis MLflow"""
        models = {}
        for model_name in ['lightgbm_ma', 'xgboost_growth']:
            model_uri = f"models:/{model_name}/Production"
            models[model_name] = mlflow.pyfunc.load_model(model_uri)
        return models
    
    def promote_model(self, model_name: str, version: str):
        """Promeut un modÃ¨le en production"""
        client = mlflow.MlflowClient()
        client.transition_model_version_stage(
            name=model_name, version=version, stage="Production"
        )
```

## ğŸ”„ API de DÃ©clenchement Manuel

```python
# api/app/api/routes/ml.py
@router.post("/ml/trigger-scoring")
async def trigger_ml_scoring(
    current_user: User = Depends(get_admin_user)
):
    """DÃ©clenche le calcul ML manuellement"""
    task = run_ml_scoring.delay()
    return {
        "task_id": task.id,
        "status": "started",
        "message": "ML scoring task initiated"
    }

@router.get("/ml/scoring-status/{task_id}")
async def get_scoring_status(task_id: str):
    """VÃ©rifie le statut d'une tÃ¢che ML"""
    task = run_ml_scoring.AsyncResult(task_id)
    return {
        "task_id": task_id,
        "status": task.status,
        "result": task.result
    }
```

## ğŸ“ˆ Monitoring et ObservabilitÃ©

### MÃ©triques ML
```python
# ml-service/app/monitoring/metrics.py
from prometheus_client import Counter, Histogram

ml_predictions_total = Counter(
    'ml_predictions_total', 
    'Total ML predictions made'
)

ml_prediction_duration = Histogram(
    'ml_prediction_duration_seconds',
    'Time spent on ML predictions'
)
```

## ğŸ”§ Configuration par Environnement

```python
# shared/config/settings.py
class Settings:
    # API Backend
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 8000
    
    # ML Service
    ML_BATCH_SIZE: int = 1000
    ML_MODEL_PATH: str = "/app/models"
    
    # Scheduler
    CELERY_BROKER_URL: str = "redis://localhost:6379/0"
    ML_SCHEDULE_HOUR: int = 2  # 2h du matin
    
    # Database
    SUPABASE_URL: str
    SUPABASE_KEY: str
```

## ğŸš€ Avantages de cette Architecture

1. **ğŸ”„ SÃ©paration des prÃ©occupations** - API rapide, ML optimisÃ©
2. **ğŸ“ˆ ScalabilitÃ©** - Services indÃ©pendants scalables
3. **ğŸ› ï¸ MaintenabilitÃ©** - Code modulaire et testable
4. **âš¡ Performance** - API sans latence ML
5. **ğŸ”§ FlexibilitÃ©** - DÃ©ploiement et mise Ã  jour indÃ©pendants
6. **ğŸ“Š Monitoring** - ObservabilitÃ© granulaire par service

Cette architecture permet de dÃ©marrer simple et d'Ã©voluer vers une solution microservices complÃ¨te selon les besoins de scaling.